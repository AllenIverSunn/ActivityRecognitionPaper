{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T17:30:12.587757Z",
     "start_time": "2018-05-09T17:29:57.654968Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import collections\n",
    "import scipy.io as io\n",
    "from scipy.stats import bernoulli\n",
    "import datetime\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import reduce\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "from sklearn import metrics\n",
    "# import xgboost as xgb\n",
    "# from sklearn.preprocerssing import OneHotEncoder\n",
    "from scipy.stats import bernoulli\n",
    "from sklearn.metrics import classification_report, precision_recall_fscore_support\n",
    "from torch.utils import data as Data\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import json\n",
    "import gc\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T17:30:18.593845Z",
     "start_time": "2018-05-09T17:30:12.591758Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sensor = np.loadtxt('./data/processed_data/sensor_data.txt')\n",
    "activity = np.loadtxt('./data/processed_data/activity_data.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T17:30:18.705868Z",
     "start_time": "2018-05-09T17:30:18.593845Z"
    },
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "class Denoising_AutoEncoder(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden):\n",
    "        super(Denoising_AutoEncoder, self).__init__()\n",
    "        self.n_input = n_input\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden),\n",
    "            nn.Sigmoid()\n",
    "#             nn.SELU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_input),\n",
    "            nn.Sigmoid()\n",
    "#             nn.SELU()\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        encoded = self.encoder(inputs)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "## 应该生成服从伯努利分布的随机噪声\n",
    "def make_noise(shape):\n",
    "    noise = bernoulli.rvs(0.1, size=shape)\n",
    "#     mask = np.random.random(size=shape)\n",
    "#     mask = (mask >= 0.8).astype(int)\n",
    "#     noise = noise * mask\n",
    "    return noise\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.data.fill_(0.02)\n",
    "#     print(m.weight)\n",
    "\n",
    "#     net = nn.Sequential(nn.Linear(2, 2), nn.Linear(2, 2))\n",
    "#     net.apply(init_weights)\n",
    "\n",
    "def train_d(net, learning_rate, criterion, optimizer, train_x, max_iter):\n",
    "    all_loss = []\n",
    "    loss = None\n",
    "    for i in range(max_iter):\n",
    "        optimizer.zero_grad()\n",
    "#         noise = Variable(torch.FloatTensor(make_noise(train_en_time_X.shape)))\n",
    "#         encoded, decoded = net(train_x + noise)\n",
    "        noise = make_noise(train_x.shape)\n",
    "        encoded, decoded = net(Variable(torch.FloatTensor(np.logical_or(train_x, noise))))\n",
    "#         labels = decoded\n",
    "#         target = Variable(torch.LongTensor(train_x))\n",
    "#         print(labels.shape, target.shape, labels.view(1, -1).shape, target.view(1, -1).shape)\n",
    "        loss = criterion(decoded, Variable(torch.FloatTensor(train_x)))\n",
    "        loss.backward(retain_variables=True)\n",
    "        optimizer.step()\n",
    "        if i % 2 == 0:\n",
    "            all_loss.append(loss)\n",
    "#             print('epoch: '+str(i)+' loss: '+str(loss))\n",
    "            print('\\r\\t Train Epoch: {}\\tLoss: {:.6f}'.format(\n",
    "                i, \n",
    "                loss.cpu().data[0]), end='')\n",
    "        if i % 20 == 0:\n",
    "            print()\n",
    "    return encoded, all_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T17:09:47.519448Z",
     "start_time": "2018-05-09T17:09:38.958873Z"
    },
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3_501\\lib\\site-packages\\torch\\autograd\\__init__.py:93: UserWarning: retain_variables option is deprecated and will be removed in 0.3. Use retain_graph instead.\n",
      "  warnings.warn(\"retain_variables option is deprecated and will be removed in 0.3. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Train Epoch: 0\tLoss: 279101.218750\n",
      "\t Train Epoch: 20\tLoss: 271556.468750\n",
      "\t Train Epoch: 40\tLoss: 263801.281250\n",
      "\t Train Epoch: 44\tLoss: 263526.843750"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.0001\n",
    "\n",
    "criterion = nn.MSELoss(size_average=False)\n",
    "model = Denoising_AutoEncoder(14, 1)\n",
    "optimizer = torch.optim.Adam(model.parameters(),\n",
    "                             lr=learning_rate,\n",
    "                             betas=(0.5, 0.999))\n",
    "# optimizer = torch.optim.SGD(model.parameters(),\n",
    "#                            lr = learning_rate)\n",
    "encoded, all_loss = train_d(model, learning_rate, criterion,\n",
    "                          optimizer, sensor, 45)\n",
    "## Classification Model Training\n",
    "features = encoded.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T17:09:51.811139Z",
     "start_time": "2018-05-09T17:09:51.803138Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80010, 30)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-23T23:42:36.307364Z",
     "start_time": "2018-04-23T23:42:35.395319Z"
    }
   },
   "source": [
    "day = int((60 * 60 * 24) / 30)\n",
    "X = []\n",
    "y = []\n",
    "\n",
    "i = 0\n",
    "while i < new_sensor.shape[0]:\n",
    "    X_test = new_sensor[i: i+day, :]\n",
    "    y_test = new_act[i: i+day, :]\n",
    "    X_train = np.vstack((new_sensor[:i, :], new_sensor[i+day:, :]))\n",
    "    y_train = np.vstack((new_act[:i, :], new_act[i+day:, :]))\n",
    "    X.append((X_train, X_test))\n",
    "    y.append((y_train, y_test))\n",
    "    i += day"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-14T14:13:47.109365Z",
     "start_time": "2018-04-14T14:13:47.103420Z"
    }
   },
   "source": [
    "###完成对于特征的重组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-24T00:21:42.207272Z",
     "start_time": "2018-04-23T23:43:49.564193Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rate = 0.0002\n",
    "\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "accuracys = []\n",
    "\n",
    "for i in range(len(X)):\n",
    "    ## Feature Representation\n",
    "    print('\\nTraining {}th fold:'.format(i))\n",
    "    X_train, X_test = np.asarray(X[i][0]), np.asarray(X[i][1])\n",
    "    y_train, y_test = np.asarray(y[i][0]), np.asarray(y[i][1])\n",
    "#     criterion = nn.MSELoss(size_average=True)\n",
    "#     criterion = nn.NLLLoss2d()\n",
    "    criterion = nn.MSELoss()\n",
    "    model = Denoising_AutoEncoder(42, 60)\n",
    "    model.apply(init_weights)\n",
    "    optimizer = torch.optim.RMSprop(model.parameters(),\n",
    "                                    lr=learning_rate)\n",
    "    encoded, all_loss = train(model, learning_rate, criterion,\n",
    "                              optimizer, X_train, 30)\n",
    "    ## Classification Model Training\n",
    "    features = encoded.data.numpy()\n",
    "    y_preds = []\n",
    "    for i in range(y_train.shape[1]):\n",
    "#         dt = DecisionTreeClassifier()\n",
    "#         dt.fit(X_train, y_train[:, i])\n",
    "#         y_preds.append(dt.predict(X_test))\n",
    "        rf = RandomForestClassifier(n_estimators=25, max_depth=3, max_features=3)\n",
    "        rf.fit(X_train, y_train[:, i])\n",
    "        y_preds.append(rf.predict(X_test))\n",
    "    y_preds = np.asarray(y_preds).T\n",
    "#     print(y_preds.shape)\n",
    "    \n",
    "    accuracys.append(metrics.accuracy_score(y_test, y_preds))\n",
    "    precisions.append(metrics.precision_score(y_test, y_preds, average='micro'))\n",
    "    recalls.append(metrics.recall_score(y_test, y_preds, average='micro'))\n",
    "    f1s.append(metrics.f1_score(y_test, y_preds, average='micro'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-24T00:22:03.390716Z",
     "start_time": "2018-04-24T00:22:03.378716Z"
    }
   },
   "source": [
    "print('Accuracy: {} | Precision: {} | Recall: {} | F1: {}'.format(np.mean(accuracys), np.mean(precisions), np.mean(recalls), np.mean(f1s)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 第三种方法"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 尝试使用一种类似于Word2Vec的编码方法编码前后时间信息"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 这里由于压缩的是另一个维度，意思是假设有n条数据，每条数据有m个特征，之前压缩的是m个特征，但是这里压缩的是n。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T17:30:40.191974Z",
     "start_time": "2018-05-09T17:30:40.067908Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, n_input, n_hidden):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_input = n_input\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(n_input, n_hidden),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(n_hidden, n_input),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        encoded = self.encoder(inputs)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return encoded, decoded\n",
    "\n",
    "def make_noise(shape):\n",
    "    noise = bernoulli.rvs(0.1, size=shape)\n",
    "#     mask = np.random.random(size=shape)\n",
    "#     mask = (mask >= 0.8).astype(int)\n",
    "#     noise = noise * mask\n",
    "    return noise\n",
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        m.weight.data.fill_(0.02)\n",
    "\n",
    "def train(net, learning_rate, criterion, optimizer, train_x, max_iter):\n",
    "    all_loss = []\n",
    "    loss = None\n",
    "    for i in range(max_iter):\n",
    "        optimizer.zero_grad()\n",
    "        noise = make_noise(train_x.shape)\n",
    "        encoded, decoded = net(Variable(torch.FloatTensor(np.logical_or(train_x, noise))))\n",
    "#         encoded, decoded = net(Variable(torch.FloatTensor(train_x)))\n",
    "#         labels = decoded\n",
    "#         target = Variable(torch.LongTensor(train_x))\n",
    "#         print(labels.shape, target.shape, labels.view(1, -1).shape, target.view(1, -1).shape)\n",
    "        loss = criterion(decoded, Variable(torch.FloatTensor(train_x)))\n",
    "        loss.backward(retain_variables=True)\n",
    "        optimizer.step()\n",
    "        if i % 2 == 0:\n",
    "            all_loss.append(loss)\n",
    "#             print('epoch: '+str(i)+' loss: '+str(loss))\n",
    "            print('\\r\\t Train Epoch: {}\\tLoss: {:.6f}'.format(\n",
    "                i, \n",
    "                loss.cpu().data[0]))\n",
    "\n",
    "    return encoded, all_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80010, 14)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T17:30:44.399684Z",
     "start_time": "2018-05-09T17:30:43.067535Z"
    }
   },
   "outputs": [],
   "source": [
    "# ## 划分窗口大小\n",
    "# window_size = 7\n",
    "\n",
    "# # print(sensor.shape, (sensor.shape[0], padding))\n",
    "# padded_activity = np.hstack((np.zeros((sensor.shape[0], padding)),\n",
    "#                              np.hstack((sensor, np.zeros((sensor.shape[0], padding))))))\n",
    "# # print(sensor.shape, padded_activity.shape)\n",
    "# sensor_feature = np.zeros(((window_size - 1) * sensor.shape[0], sensor.shape[1]))\n",
    "# print(sensor_feature.shape)\n",
    "# for i in range(padding, padding + sensor_feature.shape[1]):\n",
    "# #     print(padded_activity[:, (i-(window_size//2)):i].T.shape,\n",
    "# #           padded_activity[:, (i+1):(i+(window_size//2)+1)].T.shape)\n",
    "#     sensor_feature[:, i-padding] = np.vstack((padded_activity[:, (i-(window_size//2)):i].T,\n",
    "#                                       padded_activity[:, (i+1):(i+(window_size//2)+1)].T)).reshape(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5000, 224)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## 划分窗口数据\n",
    "window_size = 16\n",
    "\n",
    "window_data = np.zeros((window_size * sensor.shape[1], sensor.shape[0] // window_size))\n",
    "\n",
    "for i in range(window_data.shape[1]):\n",
    "    window_data[:, i] = sensor[i * window_size:(i + 1) * window_size, :].ravel()\n",
    "window_data = window_data.T\n",
    "window_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T17:31:24.996577Z",
     "start_time": "2018-05-09T17:30:57.712999Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\Anaconda3_501\\lib\\site-packages\\torch\\autograd\\__init__.py:93: UserWarning: retain_variables option is deprecated and will be removed in 0.3. Use retain_graph instead.\n",
      "  warnings.warn(\"retain_variables option is deprecated and will be removed in 0.3. \"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t Train Epoch: 0\tLoss: 0.252957\n",
      "\t Train Epoch: 2\tLoss: 0.248041\n",
      "\t Train Epoch: 4\tLoss: 0.245677\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.001\n",
    "\n",
    "criterion = nn.MSELoss()\n",
    "model = Encoder(window_data.shape[1], 28)\n",
    "model.apply(init_weights)\n",
    "optimizer = torch.optim.RMSprop(model.parameters(),\n",
    "                                lr=learning_rate)\n",
    "encoded, all_loss = train(model, learning_rate, criterion,\n",
    "                          optimizer, window_data, 5)\n",
    "## Classification Model Training\n",
    "new_features = encoded.data.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T17:31:31.998465Z",
     "start_time": "2018-05-09T17:31:31.986464Z"
    }
   },
   "outputs": [],
   "source": [
    "f = 0\n",
    "for i in range(5000):\n",
    "    if (new_features[i, :] != new_features[0, :]).any():\n",
    "        f += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T17:31:34.178627Z",
     "start_time": "2018-05-09T17:31:34.170627Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-05-09T17:37:35.756459Z",
     "start_time": "2018-05-09T17:32:36.976374Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training 0th fold:\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Number of labels=77130 does not match number of samples=2120",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-25-744a43023c57>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     17\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_train\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     18\u001b[0m         \u001b[0mrf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mRandomForestClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_estimators\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m50\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_depth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmax_features\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 19\u001b[1;33m         \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     20\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mrf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     21\u001b[0m         \u001b[0my_preds\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3_501\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[0;32m    326\u001b[0m                     \u001b[0mt\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrees\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    327\u001b[0m                     verbose=self.verbose, class_weight=self.class_weight)\n\u001b[1;32m--> 328\u001b[1;33m                 for i, t in enumerate(trees))\n\u001b[0m\u001b[0;32m    329\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    330\u001b[0m             \u001b[1;31m# Collect newly grown trees\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3_501\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m    777\u001b[0m             \u001b[1;31m# was dispatched. In particular this covers the edge\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    778\u001b[0m             \u001b[1;31m# case of Parallel used with an exhausted iterator.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 779\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    780\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3_501\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    623\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    624\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 625\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    626\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3_501\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    586\u001b[0m         \u001b[0mdispatch_timestamp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    587\u001b[0m         \u001b[0mcb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mBatchCompletionCallBack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdispatch_timestamp\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 588\u001b[1;33m         \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    589\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3_501\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    109\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    110\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 111\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    112\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    113\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3_501\\lib\\site-packages\\sklearn\\externals\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    330\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    331\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 332\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    333\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    334\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3_501\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3_501\\lib\\site-packages\\sklearn\\externals\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 131\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    132\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    133\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__len__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3_501\\lib\\site-packages\\sklearn\\ensemble\\forest.py\u001b[0m in \u001b[0;36m_parallel_build_trees\u001b[1;34m(tree, forest, X, y, sample_weight, tree_idx, n_trees, verbose, class_weight)\u001b[0m\n\u001b[0;32m    119\u001b[0m             \u001b[0mcurr_sample_weight\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mcompute_sample_weight\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'balanced'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    120\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 121\u001b[1;33m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcurr_sample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    122\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    123\u001b[0m         \u001b[0mtree\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3_501\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    788\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    789\u001b[0m             \u001b[0mcheck_input\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcheck_input\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 790\u001b[1;33m             X_idx_sorted=X_idx_sorted)\n\u001b[0m\u001b[0;32m    791\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    792\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Anaconda3_501\\lib\\site-packages\\sklearn\\tree\\tree.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, check_input, X_idx_sorted)\u001b[0m\n\u001b[0;32m    234\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[0mn_samples\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    235\u001b[0m             raise ValueError(\"Number of labels=%d does not match \"\n\u001b[1;32m--> 236\u001b[1;33m                              \"number of samples=%d\" % (len(y), n_samples))\n\u001b[0m\u001b[0;32m    237\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmin_weight_fraction_leaf\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0.5\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    238\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"min_weight_fraction_leaf must in [0, 0.5]\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Number of labels=77130 does not match number of samples=2120"
     ]
    }
   ],
   "source": [
    "day = int(60 * 60 * 24 / 30)\n",
    "l = sensor.shape[0] // day\n",
    "# print(l)\n",
    "precisions = []\n",
    "recalls = []\n",
    "f1s = []\n",
    "accuracys = []\n",
    "\n",
    "for i in range(l):\n",
    "    ## Feature Representation\n",
    "    print('\\nTraining {}th fold:'.format(i))\n",
    "    X_test = new_features[i*day:(i+1)*day, :]\n",
    "    X_train = np.vstack((new_features[:i*day, :], new_features[(i+1)*day:, :]))\n",
    "    y_test = activity[i*day:(i+1)*day, :]\n",
    "    y_train = np.vstack((activity[:i*day, :], activity[(i+1)*day:, :]))  \n",
    "    y_preds = []\n",
    "    for i in range(y_train.shape[1]):\n",
    "        rf = RandomForestClassifier(n_estimators=50, max_depth=3, max_features=3)\n",
    "        rf.fit(X_train, y_train[:, i])\n",
    "        y_pred = rf.predict(X_test)\n",
    "        y_preds.append(y_pred) \n",
    "    y_preds = np.asarray(y_preds).T    \n",
    "    \n",
    "    accuracy = metrics.accuracy_score(y_test, y_preds)\n",
    "    precision = metrics.precision_score(y_test, y_preds, average='micro')\n",
    "    recall = metrics.recall_score(y_test, y_preds, average='micro')\n",
    "    f1 = metrics.f1_score(y_test, y_preds, average='micro')\n",
    "    print('\\tAccuracy: {} | Precision: {} | Recall: {} | F1: {}'.format(accuracy,\n",
    "                                                                        precision,\n",
    "                                                                        recall,\n",
    "                                                                        f1))\n",
    "    \n",
    "    accuracys.append(accuracy)\n",
    "    precisions.append(precision)\n",
    "    recalls.append(recall)\n",
    "    f1s.append(f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Accuracy: {} | Precision: {} | Recall: {} | F1: {}'.format(np.mean(accuracys),\n",
    "                                                                  np.mean(precisions),\n",
    "                                                                  np.mean(recalls),\n",
    "                                                                  np.mean(f1s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-25T02:16:28.982876Z",
     "start_time": "2018-04-25T02:16:28.972867Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.7601851851851853 | Precision: 0.7703532657248031 | Recall: 0.872567805642764 | F1: 0.8149614643342248\n"
     ]
    }
   ],
   "source": [
    "print('Accuracy: {} | Precision: {} | Recall: {} | F1: {}'.format(np.mean(accuracys),\n",
    "                                                                  np.mean(precisions),\n",
    "                                                                  np.mean(recalls),\n",
    "                                                                  np.mean(f1s)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-04-24T19:15:39.829913Z",
     "start_time": "2018-04-24T19:15:39.806898Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "?xgb.XGBClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "做其他算法的precision-recall用作对比"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb = GaussianNB()\n",
    "nb.fit(sensor, activity)\n",
    "nb_pred = nb.pre"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
